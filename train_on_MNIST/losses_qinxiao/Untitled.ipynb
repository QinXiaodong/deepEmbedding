{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 32\n",
    "input_dim = 3\n",
    "output_dim = 2\n",
    "num_class = 4\n",
    "x = Variable(torch.rand(data_size, input_dim), requires_grad=False)\n",
    "w = Variable(torch.rand(input_dim, output_dim), requires_grad=True)\n",
    "inputs = x.mm(w)\n",
    "y_ = 8*list(range(num_class))\n",
    "targets = Variable(torch.IntTensor(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=np.random.rand(data_size,input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=np.array(8*list(range(num_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('inputs.npy',inputs)\n",
    "np.save('targets.npy',targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3324, 0.6856, 0.1523],\n",
       "        [0.7529, 0.7688, 0.6183],\n",
       "        [0.2742, 0.2200, 0.9693],\n",
       "        [0.4818, 0.5265, 0.8199],\n",
       "        [0.9567, 0.6536, 0.7073],\n",
       "        [0.1343, 0.0573, 0.3201],\n",
       "        [0.5711, 0.7084, 0.4961],\n",
       "        [0.1555, 0.7137, 0.9226],\n",
       "        [0.8204, 0.1300, 0.8127],\n",
       "        [0.9781, 0.2432, 0.8335],\n",
       "        [0.5221, 0.4624, 0.9861],\n",
       "        [0.4376, 0.8552, 0.6484],\n",
       "        [0.8922, 0.0657, 0.1166],\n",
       "        [0.7064, 0.5949, 0.1227],\n",
       "        [0.0469, 0.9482, 0.6797],\n",
       "        [0.1598, 0.9735, 0.6525],\n",
       "        [0.3287, 0.8011, 0.2352],\n",
       "        [0.7624, 0.4998, 0.2242],\n",
       "        [0.8136, 0.8486, 0.2725],\n",
       "        [0.2237, 0.8923, 0.6831],\n",
       "        [0.5758, 0.4865, 0.0819],\n",
       "        [0.7716, 0.8008, 0.0637],\n",
       "        [0.4058, 0.8306, 0.7508],\n",
       "        [0.5420, 0.4746, 0.9838],\n",
       "        [0.7328, 0.3135, 0.9274],\n",
       "        [0.6032, 0.6289, 0.3127],\n",
       "        [0.2992, 0.6876, 0.5736],\n",
       "        [0.4998, 0.7625, 0.1210],\n",
       "        [0.0325, 0.0674, 0.0866],\n",
       "        [0.0953, 0.4300, 0.8032],\n",
       "        [0.3126, 0.8290, 0.8315],\n",
       "        [0.7491, 0.1111, 0.4400]], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=torch.zeros(targets.shape[0], targets.max()+1).scatter_(1,torch.unsqueeze(targets.long(),1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=torch.from_numpy(np.load('targets.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_instances_per_appeared_class=torch.unsqueeze(targets.sum(0),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_instances_per_appeared_class=targets.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.0000, 8.0000, 8.0000, 8.0000])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_instances_per_appeared_class[num_of_instances_per_appeared_class!=0]+=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.rand(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=torch.zeros(targets.shape[0], targets.max()+1).scatter_(1,torch.unsqueeze(targets.long(),1), 1)#转成one_hot编码\n",
    "num_of_instances_per_appeared_class=torch.unsqueeze(targets.sum(0),1)\n",
    "num_of_instances_per_appeared_class[num_of_instances_per_appeared_class==0]=1e-6#避免除零操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.],\n",
       "        [8.],\n",
       "        [8.],\n",
       "        [8.]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_instances_per_appeared_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposedTargets=targets.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_embeddings_per_appeared_class=transposedTargets.mm(inputs.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embedding_per_appeared_class=sum_of_embeddings_per_appeared_class/num_of_instances_per_appeared_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_diff_per_instance=(inputs.float()-torch.index_select(input=mean_embedding_per_appeared_class,dim=0,index=targets.long()))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_square_diff_per_appeared_class=torch.mm(transposedTargets,square_diff_per_instance).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_square_diff_per_appeared_class=torch.unsqueeze(sum_of_square_diff_per_appeared_class,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2840],\n",
       "        [0.2240],\n",
       "        [0.1490],\n",
       "        [0.1756]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_of_embeddings_per_appeared_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_of_embeddings_per_appeared_class=sum_of_square_diff_per_appeared_class/num_of_instances_per_appeared_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import pairwise_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7321e-06, 1.7321e-06, 1.7321e-06, 1.7321e-06])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_distance(mean_embedding_per_appeared_class,mean_embedding_per_appeared_class,p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanded_pairwise_distances(x, y=None):\n",
    "    '''\n",
    "    Input: x is a Nxd matrix\n",
    "           y is an optional Mxd matirx\n",
    "    Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
    "            if y is not given then use 'y=x'.\n",
    "    i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
    "    '''\n",
    "    if y is not None:\n",
    "         differences = x.unsqueeze(1) - y.unsqueeze(0)\n",
    "    else:\n",
    "        differences = x.unsqueeze(1) - x.unsqueeze(0)\n",
    "    distances = torch.sum(differences * differences, -1)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_loss_matrix=expanded_pairwise_distances(mean_embedding_per_appeared_class)\n",
    "intra_loss_matrix=var_of_embeddings_per_appeared_class+var_of_embeddings_per_appeared_class.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0113, 0.2097, 0.1732],\n",
       "        [0.0113, 0.0000, 0.1535, 0.1244],\n",
       "        [0.2097, 0.1535, 0.0000, 0.0021],\n",
       "        [0.1732, 0.1244, 0.0021, 0.0000]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_loss_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2097, 0.1535, 0.2097, 0.1732]), tensor([2, 2, 0, 0]))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_loss_matrix.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5680, 1.4967, 1.3225, 1.3511]), tensor([0, 0, 3, 3]))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(intra_loss_matrix-inter_loss_matrix+1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_loss,_=inter_loss_matrix.max(0)\n",
    "intra_loss=var_of_embeddings_per_appeared_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2840],\n",
       "        [0.2240],\n",
       "        [0.1490],\n",
       "        [0.1756]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intra_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_matrix=intra_loss_matrix-inter_loss_matrix+1\n",
    "loss_matrix[loss_matrix<0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_matrix=loss_matrix-torch.eye(loss_matrix.shape[0])*loss_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -inf, 0.4033, 0.2016, 0.2518],\n",
       "        [0.4033,   -inf, 0.1984, 0.2430],\n",
       "        [0.2016, 0.1984,   -inf, 0.2795],\n",
       "        [0.2518, 0.2430, 0.2795,   -inf]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(loss_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(inputs_):\n",
    "    # Compute pairwise distance, replace by the official when merged\n",
    "    n = inputs_.size(0)\n",
    "    dist = torch.pow(inputs_, 2).sum(dim=1, keepdim=True).expand(n, n)\n",
    "    dist = dist + dist.t()\n",
    "    dist.addmm_(1, -2, inputs_, inputs_.t())\n",
    "    dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " euclidean_dist(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is  tensor([[0.7918, 0.8124, 0.4573],\n",
      "        [0.6980, 0.1915, 0.4943],\n",
      "        [0.0552, 0.7625, 0.6164],\n",
      "        [0.3715, 0.2368, 0.8934],\n",
      "        [0.4227, 0.3490, 0.9166],\n",
      "        [0.4253, 0.4417, 0.1607],\n",
      "        [0.0072, 0.8039, 0.9495],\n",
      "        [0.1067, 0.6162, 0.5311],\n",
      "        [0.8616, 0.6075, 0.4199],\n",
      "        [0.4528, 0.7117, 0.4139],\n",
      "        [0.2457, 0.8873, 0.3406],\n",
      "        [0.7230, 0.8553, 0.6650],\n",
      "        [0.9767, 0.0203, 0.0876],\n",
      "        [0.0785, 0.4098, 0.7339],\n",
      "        [0.6217, 0.1393, 0.5839],\n",
      "        [0.7409, 0.0082, 0.8689],\n",
      "        [0.8467, 0.2072, 0.4078],\n",
      "        [0.4747, 0.2405, 0.6764],\n",
      "        [0.5084, 0.2390, 0.8339],\n",
      "        [0.7019, 0.2145, 0.9423],\n",
      "        [0.3267, 0.9125, 0.8815],\n",
      "        [0.2173, 0.4135, 0.7762],\n",
      "        [0.7723, 0.8599, 0.7857],\n",
      "        [0.0903, 0.7007, 0.0624],\n",
      "        [0.6417, 0.0987, 0.2302],\n",
      "        [0.3098, 0.2549, 0.2843],\n",
      "        [0.7674, 0.9477, 0.8390],\n",
      "        [0.2776, 0.3942, 0.9080],\n",
      "        [0.6597, 0.8117, 0.5931],\n",
      "        [0.4336, 0.6292, 0.5470],\n",
      "        [0.3968, 0.0472, 0.3204],\n",
      "        [0.4309, 0.3576, 0.9945]])\n",
      "initial parameters are  tensor([[0.4987, 0.9149],\n",
      "        [0.3100, 0.4566],\n",
      "        [0.1858, 0.0519]], requires_grad=True)\n",
      "extracted feature is : tensor([[0.7317, 1.1191],\n",
      "        [0.4993, 0.7516],\n",
      "        [0.3784, 0.4306],\n",
      "        [0.4247, 0.4944],\n",
      "        [0.4893, 0.5936],\n",
      "        [0.3789, 0.5991],\n",
      "        [0.4292, 0.4229],\n",
      "        [0.3429, 0.4065],\n",
      "        [0.6960, 1.0874],\n",
      "        [0.5234, 0.7607],\n",
      "        [0.4609, 0.6476],\n",
      "        [0.7493, 1.0864],\n",
      "        [0.5097, 0.9073],\n",
      "        [0.3025, 0.2970],\n",
      "        [0.4618, 0.6627],\n",
      "        [0.5334, 0.7266],\n",
      "        [0.5623, 0.8904],\n",
      "        [0.4370, 0.5792],\n",
      "        [0.4826, 0.6175],\n",
      "        [0.5916, 0.7889],\n",
      "        [0.6096, 0.7613],\n",
      "        [0.3808, 0.4279],\n",
      "        [0.7977, 1.1399],\n",
      "        [0.2738, 0.4057],\n",
      "        [0.3934, 0.6440],\n",
      "        [0.2864, 0.4146],\n",
      "        [0.8324, 1.1783],\n",
      "        [0.4294, 0.4811],\n",
      "        [0.6909, 1.0049],\n",
      "        [0.5130, 0.7124],\n",
      "        [0.2720, 0.4012],\n",
      "        [0.5106, 0.6091]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "data_size = 32\n",
    "input_dim = 3\n",
    "output_dim = 2\n",
    "num_class = 3\n",
    "an_margin = 0.7\n",
    "ap_margin = 0.3\n",
    "x = Variable(torch.rand(data_size, input_dim), requires_grad=False)\n",
    "w = Variable(torch.rand(input_dim, output_dim), requires_grad=True)\n",
    "print('training data is ', x)\n",
    "print('initial parameters are ', w)\n",
    "inputs = x.mm(w)\n",
    "print('extracted feature is :', inputs)\n",
    "y_ = np.random.randint(num_class, size=data_size)\n",
    "targets = Variable(torch.from_numpy(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\paoso\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py:263: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "n = inputs.size(0)\n",
    "num_dim = inputs.size(1)\n",
    "targets_ = list(set(targets.data))\n",
    "num_class = len(targets_)\n",
    "\n",
    "targets_ = Variable(torch.LongTensor(targets_))\n",
    "mask_ = targets.repeat(num_class, 1).eq(targets_.int().repeat(n, 1).t())\n",
    "centers = []\n",
    "inputs_list = []\n",
    "\n",
    "# calculate the centers for every class in one mini-batch\n",
    "for i, target in enumerate(targets_):\n",
    "    mask_i = mask_[i].repeat(num_dim, 1).t()\n",
    "    input_ = inputs[mask_i].resize(len(inputs[mask_i]) // num_dim, num_dim)\n",
    "    centers.append(torch.mean(input_, 0))\n",
    "    inputs_list.append(input_)\n",
    "\n",
    "centers = [centers[i].resize(1, num_dim) for i in range(len(centers))]\n",
    "centers = torch.cat(centers, 0)\n",
    "\n",
    "# compute centers loss here\n",
    "dist_ap, dist_an = [], []\n",
    "centers_dist = euclidean_dist(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5161, 0.7041],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.4525, 0.6165],\n",
       "        [0.4525, 0.6165],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.4525, 0.6165],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.5534, 0.7989],\n",
       "        [0.4525, 0.6165],\n",
       "        [0.4525, 0.6165],\n",
       "        [0.4525, 0.6165],\n",
       "        [0.4525, 0.6165],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.5534, 0.7989],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.5534, 0.7989],\n",
       "        [0.5534, 0.7989],\n",
       "        [0.4525, 0.6165],\n",
       "        [0.5534, 0.7989],\n",
       "        [0.5534, 0.7989],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.4525, 0.6165],\n",
       "        [0.4525, 0.6165],\n",
       "        [0.4525, 0.6165],\n",
       "        [0.5161, 0.7041],\n",
       "        [0.4525, 0.6165]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.],\n",
       "        [8.],\n",
       "        [1.],\n",
       "        [8.]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_instances_per_appeared_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embedding_per_appeared_class.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7317, 1.1191],\n",
       "        [0.4993, 0.7516],\n",
       "        [0.3784, 0.4306],\n",
       "        [0.4247, 0.4944],\n",
       "        [0.4893, 0.5936],\n",
       "        [0.3789, 0.5991],\n",
       "        [0.4292, 0.4229],\n",
       "        [0.3429, 0.4065],\n",
       "        [0.6960, 1.0874],\n",
       "        [0.5234, 0.7607],\n",
       "        [0.4609, 0.6476],\n",
       "        [0.7493, 1.0864],\n",
       "        [0.5097, 0.9073],\n",
       "        [0.3025, 0.2970],\n",
       "        [0.4618, 0.6627],\n",
       "        [0.5334, 0.7266],\n",
       "        [0.5623, 0.8904],\n",
       "        [0.4370, 0.5792],\n",
       "        [0.4826, 0.6175],\n",
       "        [0.5916, 0.7889],\n",
       "        [0.6096, 0.7613],\n",
       "        [0.3808, 0.4279],\n",
       "        [0.7977, 1.1399],\n",
       "        [0.2738, 0.4057],\n",
       "        [0.3934, 0.6440],\n",
       "        [0.2864, 0.4146],\n",
       "        [0.8324, 1.1783],\n",
       "        [0.4294, 0.4811],\n",
       "        [0.6909, 1.0049],\n",
       "        [0.5130, 0.7124],\n",
       "        [0.2720, 0.4012],\n",
       "        [0.5106, 0.6091]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.from_numpy(np.load('inputs.npy'))\n",
    "targets=torch.from_numpy(np.load('targets.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3,\n",
       "        0, 1, 2, 3, 0, 1, 2, 3], dtype=torch.int32)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=inputs.cpu().float()\n",
    "targets=targets.cpu().long()\n",
    "one_hot_targets=torch.zeros(targets.shape[0], targets.max()+1).scatter_(1,torch.unsqueeze(targets,1), 1)#转成one_hot编码\n",
    "num_of_instances_per_appeared_class=torch.unsqueeze(one_hot_targets.sum(0),1)\n",
    "# num_of_instances_per_appeared_class[num_of_instances_per_appeared_class==0]=1e-6#避免除零操作\n",
    "\n",
    "transposedTargets=one_hot_targets.t()\n",
    "sum_of_embeddings_per_appeared_class=torch.mm(transposedTargets,inputs)\n",
    "mean_embedding_per_appeared_class=sum_of_embeddings_per_appeared_class/(num_of_instances_per_appeared_class+1e-6)\n",
    "\n",
    "mean_embedding_all=inputs.mean(0)\n",
    "temp=mean_embedding_per_appeared_class-mean_embedding_all\n",
    "S_b=torch.mm((temp*num_of_instances_per_appeared_class).t(),temp)\n",
    "\n",
    "temp2=inputs-mean_embedding_all\n",
    "S_t=torch.mm(temp2.t(),temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4101, -0.5024, -0.3519],\n",
       "        [-0.5024,  2.4872,  0.0395],\n",
       "        [-0.3519,  0.0395,  3.1118]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=torch.zeros(inputs.shape[1],inputs.shape[1])\n",
    "for i in range(inputs.shape[0]):\n",
    "    temp=(inputs[i,:]-mean_embedding_all).unsqueeze(0)\n",
    "    res+=temp.t().mm(temp)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0849, -0.1643, -0.1491]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean_embedding_per_appeared_class[i,:]-mean_embedding_all).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3739)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0/torch.trace(torch.mm(S_b,torch.inverse(S_t+1e-6*torch.eye(S_t.shape[0]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
